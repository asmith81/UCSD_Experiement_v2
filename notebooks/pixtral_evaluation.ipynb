{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb964086",
   "metadata": {},
   "source": [
    "# Pixtral Model Evaluation\n",
    "\n",
    "This notebook evaluates the Pixtral-12B model's performance across different quantization levels\n",
    "and prompt strategies for invoice data extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e373c9",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "### Import system dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f570df42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import json\n",
    "\n",
    "# Install dependencies\n",
    "print(\"Installing dependencies...\")\n",
    "try:\n",
    "    # Install base requirements first\n",
    "    print(\"Installing base requirements...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-r\", str(ROOT_DIR / \"requirements.txt\")])\n",
    "    print(\"Base requirements installed successfully.\")\n",
    "\n",
    "        # Install PyTorch dependencies separately\n",
    "    print(\"Installing PyTorch dependencies...\")\n",
    "    subprocess.check_call([\n",
    "        sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
    "        \"torch==2.1.0\",\n",
    "        \"torchvision==0.16.0\",\n",
    "        \"torchaudio==2.1.0\",\n",
    "        \"--index-url\", \"https://download.pytorch.org/whl/cu118\"\n",
    "    ])\n",
    "    print(\"PyTorch dependencies installed successfully.\")\n",
    "    \n",
    "    # Install AI-specific dependencies\n",
    "    print(\"Installing AI-specific dependencies...\")\n",
    "    subprocess.check_call([\n",
    "        sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
    "        \"transformers==4.50.3\",\n",
    "        \"accelerate>=0.26.0\",\n",
    "        \"bitsandbytes==0.45.5\",\n",
    "        \"huggingface_hub>=0.20.3\",\n",
    "        \"flash-attn==2.5.0\"\n",
    "    ])\n",
    "    print(\"AI-specific dependencies installed successfully.\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    logger.error(f\"Error installing dependencies: {e}\")\n",
    "    raise\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33fa0ab",
   "metadata": {},
   "source": [
    "### Configure Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43282031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging \n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fe8e0f",
   "metadata": {},
   "source": [
    "### Determine Root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70efb9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine root directory\n",
    "try:\n",
    "    # When running as a script\n",
    "    ROOT_DIR = Path(__file__).parent.parent\n",
    "except NameError:\n",
    "    # When running in a notebook, look for project root markers\n",
    "    current_dir = Path.cwd()\n",
    "    while current_dir != current_dir.parent:\n",
    "        if (current_dir / 'src').exists() and (current_dir / 'notebooks').exists():\n",
    "            ROOT_DIR = current_dir\n",
    "            break\n",
    "        current_dir = current_dir.parent\n",
    "    else:\n",
    "        raise RuntimeError(\"Could not find project root directory. Make sure you're running from within the project structure.\")\n",
    "\n",
    "sys.path.append(str(ROOT_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dffc30",
   "metadata": {},
   "source": [
    "### Setup Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240dc768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Paths\n",
    "try:\n",
    "    # Define base directories\n",
    "    env = {\n",
    "        'data_dir': ROOT_DIR / 'data',\n",
    "        'models_dir': ROOT_DIR / 'models',\n",
    "        'logs_dir': ROOT_DIR / 'logs',\n",
    "        'results_dir': ROOT_DIR / 'results',\n",
    "        'prompts_dir': ROOT_DIR / 'prompts',\n",
    "        'config_dir': ROOT_DIR / 'config'\n",
    "    }\n",
    "    \n",
    "    # Validate paths\n",
    "    required_paths = ['data_dir', 'models_dir', 'logs_dir', 'results_dir', 'prompts_dir', 'config_dir']\n",
    "    missing_paths = [path for path in required_paths if path not in env]\n",
    "    if missing_paths:\n",
    "        raise RuntimeError(f\"Missing required paths in environment: {missing_paths}\")\n",
    "        \n",
    "    # Ensure required directories exist\n",
    "    for path in required_paths:\n",
    "        env[path].mkdir(parents=True, exist_ok=True)\n",
    "        logger.info(f\"Created/verified directory: {env[path]}\")\n",
    "    \n",
    "    # Create subdirectories for results\n",
    "    (env['results_dir'] / 'raw_results').mkdir(parents=True, exist_ok=True)\n",
    "    (env['results_dir'] / 'processed_results').mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    logger.info(\"All required directories have been set up successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error setting up environment: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3c81af",
   "metadata": {},
   "source": [
    "### Import Local Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836f4365",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import project modules\n",
    "from src import execution\n",
    "from src.environment import setup_environment, download_model\n",
    "from src.config import load_yaml_config\n",
    "from src.models.llama_vision import load_model, process_image_wrapper, download_llama_vision_model\n",
    "from src.prompts import load_prompt_template\n",
    "from src.results_logging import track_execution, log_result, ResultStructure, evaluate_model_output\n",
    "from src.validation import validate_results\n",
    "from src.data_utils import DataConfig, setup_data_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9271b88c",
   "metadata": {},
   "source": [
    "### Confugure Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9221b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load configuration\n",
    "config_path = ROOT_DIR / \"config\" / \"models\" / \"pixtral.yaml\"\n",
    "if not config_path.exists():\n",
    "    raise FileNotFoundError(f\"Configuration file not found: {config_path}\")\n",
    "\n",
    "try:\n",
    "    config = load_yaml_config(str(config_path))\n",
    "    # Validate required configuration sections\n",
    "    required_sections = ['name', 'loading', 'quantization', 'prompt', 'inference']\n",
    "    missing_sections = [section for section in required_sections if section not in config]\n",
    "    if missing_sections:\n",
    "        raise ValueError(f\"Configuration missing required sections: {missing_sections}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading configuration: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# Setup data configuration\n",
    "try:\n",
    "    data_config = setup_data_paths(\n",
    "        env_config=env,\n",
    "        image_extensions=['.jpg', '.jpeg', '.png'],\n",
    "        max_image_size=1120,\n",
    "        supported_formats=['RGB', 'L']\n",
    "    )\n",
    "    logger.info(\"Data configuration setup successfully\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error setting up data configuration: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# Load model configuration\n",
    "try:\n",
    "    # The config is already loaded and validated with required sections\n",
    "    # We can use the config directly as it matches our needs\n",
    "    model_config = {\n",
    "        'name': config['name'],\n",
    "        'path': config['repo_id'],\n",
    "        'quantization_levels': list(config['quantization']['options'].keys())\n",
    "    }\n",
    "    \n",
    "    prompt_config = {\n",
    "        'format': config['prompt']['format'],\n",
    "        'image_placeholder': config['prompt']['image_placeholder'],\n",
    "        'default_field': config['prompt']['default_field']\n",
    "    }\n",
    "    \n",
    "    # Validate model configuration\n",
    "    required_model_fields = ['name', 'path', 'quantization_levels']\n",
    "    missing_fields = [field for field in required_model_fields if field not in model_config]\n",
    "    if missing_fields:\n",
    "        raise ValueError(f\"Model configuration missing required fields: {missing_fields}\")\n",
    "        \n",
    "except KeyError as e:\n",
    "    logger.error(f\"Missing required configuration section: {e}\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading model configuration: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "print(f\"✓ Model configuration loaded successfully for {MODEL_NAME}\")\n",
    "\n",
    "# Set model for this notebook\n",
    "MODEL_NAME = \"pixtral\"\n",
    "TEST_MATRIX_PATH = str(ROOT_DIR / \"config\" / \"test_matrix.json\")\n",
    "EXECUTION_LOG_PATH = env['logs_dir'] / f\"{MODEL_NAME}_execution.log\"\n",
    "\n",
    "# Validate test matrix exists and is valid\n",
    "try:\n",
    "    if not Path(TEST_MATRIX_PATH).exists():\n",
    "        raise FileNotFoundError(f\"Test matrix file not found: {TEST_MATRIX_PATH}\")\n",
    "        \n",
    "    # Load and validate test matrix\n",
    "    with open(TEST_MATRIX_PATH, 'r') as f:\n",
    "        test_matrix = json.load(f)\n",
    "        \n",
    "    # Validate test matrix structure\n",
    "    if 'test_cases' not in test_matrix:\n",
    "        raise ValueError(\"Test matrix must contain 'test_cases' array\")\n",
    "        \n",
    "    # Validate required fields\n",
    "    required_fields = ['model_name', 'field_type', 'prompt_type', 'quant_level', 'image_path']\n",
    "    for test_case in test_matrix['test_cases']:\n",
    "        missing_fields = [field for field in required_fields if field not in test_case]\n",
    "        if missing_fields:\n",
    "            raise ValueError(f\"Test case missing required fields: {missing_fields}\")\n",
    "            \n",
    "    # Validate quantization values\n",
    "    valid_quantization = [4, 8, 16, 32]\n",
    "    invalid_quantization = [case['quant_level'] for case in test_matrix['test_cases'] \n",
    "                          if case['quant_level'] not in valid_quantization]\n",
    "    if invalid_quantization:\n",
    "        raise ValueError(f\"Invalid quantization values found: {invalid_quantization}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error validating test matrix: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f902ee",
   "metadata": {},
   "source": [
    "## Test Execution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f585d69c",
   "metadata": {},
   "source": [
    "### Configure the Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d821768d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User Configuration\n",
    "print(\"Configure your test parameters:\")\n",
    "\n",
    "# Get quantization level\n",
    "while True:\n",
    "    try:\n",
    "        QUANTIZATION_LEVEL = int(input(\"Enter quantization level (4, 8, 16, or 32): \"))\n",
    "        if QUANTIZATION_LEVEL not in [4, 8, 16, 32]:\n",
    "            print(\"Invalid quantization level. Please enter 4, 8, 16, or 32.\")\n",
    "            continue\n",
    "        break\n",
    "    except ValueError:\n",
    "        print(\"Please enter a valid number.\")\n",
    "\n",
    "# Get prompt type\n",
    "available_prompts = [\n",
    "    \"basic_extraction\",  # From basic_extraction.yaml\n",
    "    \"detailed\",          # From detailed.yaml\n",
    "    \"few_shot\",         # From few_shot.yaml\n",
    "    \"locational\",       # From locational.yaml\n",
    "    \"step_by_step\"      # From step_by_step.yaml\n",
    "]\n",
    "\n",
    "print(f\"\\nAvailable prompt types:\")\n",
    "for i, prompt in enumerate(available_prompts, 1):\n",
    "    print(f\"{i}. {prompt}\")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        choice = int(input(\"\\nEnter the number of your chosen prompt type: \"))\n",
    "        if 1 <= choice <= len(available_prompts):\n",
    "            PROMPT_TYPE = available_prompts[choice - 1]\n",
    "            break\n",
    "        print(f\"Please enter a number between 1 and {len(available_prompts)}\")\n",
    "    except ValueError:\n",
    "        print(\"Please enter a valid number.\")\n",
    "\n",
    "# Set other required variables\n",
    "MODEL_NAME = \"pixtral\"\n",
    "FIELD_TYPES = [\"work_order_number\", \"total_cost\"]\n",
    "TEST_MATRIX_PATH = env['config_dir'] / \"test_matrix.json\"\n",
    "RAW_RESULTS_DIR = env['results_dir'] / \"raw_results\" / f\"{MODEL_NAME}_{QUANTIZATION_LEVEL}bit_{PROMPT_TYPE}\"\n",
    "RAW_RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"\\nTest Configuration:\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Quantization: {QUANTIZATION_LEVEL} bits\")\n",
    "print(f\"Prompt Type: {PROMPT_TYPE}\")\n",
    "print(f\"Field Types: {', '.join(FIELD_TYPES)}\")\n",
    "print(f\"Results will be saved to: {RAW_RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95fb1d1",
   "metadata": {},
   "source": [
    "### Run the Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4677abca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this before running the test suite\n",
    "for test_case in test_matrix['test_cases']:\n",
    "    image_path = Path(test_case['image_path'])\n",
    "    if not image_path.exists():\n",
    "        raise FileNotFoundError(f\"Image not found: {image_path}\")\n",
    "\n",
    "\n",
    "# Run test suite\n",
    "try:\n",
    "    # Create test matrix for this run\n",
    "    test_matrix = {\n",
    "        \"test_cases\": [\n",
    "            {\n",
    "                \"model_name\": MODEL_NAME,\n",
    "                \"field_type\": field_type,\n",
    "                \"prompt_type\": PROMPT_TYPE,\n",
    "                \"quant_level\": QUANTIZATION_LEVEL,\n",
    "                \"image_path\": str(env['data_dir'] / f\"{1017 + i}.jpg\")  # Start from 1017.jpg\n",
    "            }\n",
    "            for i, field_type in enumerate(FIELD_TYPES)\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Save test matrix\n",
    "    with open(TEST_MATRIX_PATH, 'w') as f:\n",
    "        json.dump(test_matrix, f, indent=2)\n",
    "    \n",
    "    # Run raw test suite\n",
    "    run_raw_test_suite(\n",
    "        model_loader=load_model,\n",
    "        test_matrix=test_matrix['test_cases'],\n",
    "        output_dir=RAW_RESULTS_DIR,\n",
    "        prompts_dir=env['prompts_dir']\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ Raw test execution completed successfully for:\")\n",
    "    print(f\"- Model: {MODEL_NAME}\")\n",
    "    print(f\"- Quantization: {QUANTIZATION_LEVEL} bits\")\n",
    "    print(f\"- Prompt Type: {PROMPT_TYPE}\")\n",
    "    print(f\"- Field Types: {', '.join(FIELD_TYPES)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in raw test execution: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d117cc6",
   "metadata": {},
   "source": [
    "## Phase 2: Result Processing\n",
    "\n",
    "Process the raw results and generate analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ea1589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process results\n",
    "try:\n",
    "    summary = process_results(\n",
    "        results=results,\n",
    "        output_dir=PROCESSED_RESULTS_DIR\n",
    "    )\n",
    "    print(\"✓ Result processing completed successfully\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in result processing: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b829bfa",
   "metadata": {},
   "source": [
    "## Analysis and Visualization\n",
    "\n",
    "Display the processed results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057af9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display metrics\n",
    "print(\"\\nPerformance Metrics:\")\n",
    "print(f\"Total Tests: {summary['total_tests']}\")\n",
    "print(f\"Successful Tests: {summary['successful_tests']}\")\n",
    "print(f\"Failed Tests: {summary['failed_tests']}\")\n",
    "print(f\"Average Processing Time: {summary['average_processing_time']:.2f}s\")\n",
    "\n",
    "print(\"\\nField Accuracy:\")\n",
    "for field, metrics in summary['field_accuracy'].items():\n",
    "    print(f\"{field}: {metrics['accuracy']:.2%} ({metrics['success']}/{metrics['total']})\") "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
